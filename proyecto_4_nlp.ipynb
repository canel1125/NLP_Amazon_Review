{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pueden encontrar el link al repositorio [aquí](https://github.com/canel1125/NLP_Amazon_Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retomamos el modelo del proyecto anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos Spacy y sus funciones para lemmatizar, vectorizar y modelar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos de nuevo el dataset para poder trabajar en limpio y ver como se comporta si lo normalizamos y transformamos con Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_json('dataset_amazon/dataset_es_train.json', lines = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.drop(['language','reviewer_id','product_id','review_id','product_category'],axis = 1,inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "nlp = spacy.load(\"es_core_news_sm\") # Cargamos la versión en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armamos la funciones para limpiar y procesar las oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libreria para reemplzar caracteres\n",
    "import re\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords.remove('muy')\n",
    "stopwords.remove('nada')\n",
    "stopwords.remove('poco')\n",
    "stopwords.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que remueve acentos de palabras\n",
    "def remover_acentos(palabra):\n",
    "    #Reglas o letras a cambiar\n",
    "    reglas = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ñ\", \"n\"),\n",
    "    )\n",
    "    for a, b in reglas:\n",
    "        palabra = palabra.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que tokeniza y normaliza las oraciones\n",
    "\n",
    "def token_cleaner(oracion):\n",
    "    doc = nlp(oracion)\n",
    "    temp=[]\n",
    "    for token in doc:\n",
    "        # Vamos a reemplzar los caracteres que no sean letras por espacios\n",
    "        token=remover_acentos(token.lemma_)\n",
    "        token=re.sub(\"[^a-zA-Z]\",\" \",str(token))\n",
    "        # Pasamos todo a minúsculas\n",
    "        token=token.lower()\n",
    "        # Tokenizamos para separar las palabras del titular\n",
    "        token=nltk.word_tokenize(token)\n",
    "        # Eliminamos las palabras de menos de 3 letras\n",
    "        token = [palabra for palabra in token if len(palabra)>=2]\n",
    "        # Sacamos las Stopwords\n",
    "        token = [palabra for palabra in token if not palabra in stopwords ]\n",
    "\n",
    "        ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "        \n",
    "        # Por ultimo volvemos a unir el titular\n",
    "        token=\"\".join(token)\n",
    "\n",
    "        # Agregamos a la lista la review\n",
    "        temp.append(token)\n",
    "        \n",
    "    temp = list(filter(None, temp))\n",
    "    \n",
    "    return(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para poder graficar la matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def confusion(ytest,y_pred):\n",
    "    stars=[\"1\",\"2\"]\n",
    "    cm=confusion_matrix(ytest,y_pred)\n",
    "    f,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n",
    "    plt.xlabel(\"y_pred\")\n",
    "    plt.ylabel(\"y_true\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empezamos a definir el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como calsificador vamos usar un SVC y para tokenizar TFIDF, que debería tener un mejor desempeño\n",
    "from sklearn.svm import LinearSVC\n",
    "tfidf = TfidfVectorizer(tokenizer = token_cleaner)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos, entrenamos y predecimos como siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['review_title']\n",
    "y = dataset['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armamos una matriz de confusión para ver como se desempeña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el valor para poder comparar con los siguientes modelos que vamos a entrenar cuando reduscamos la cantidad de variables a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con unos titulos basicos a ver como clasifica a mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulosdeprueba = [[\"muy malo\",0],[\"malo\",0],[\"bueno\",0], [\"medio malo\",0],[\"excelente\",0], [\"si me sirvio para lo que es\",0], [\"Buen precio calidad\",0]]\n",
    "\n",
    "for titulo in titulosdeprueba:\n",
    "    titulo[1]=clf.predict([titulo[0]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_titulosdeprueba = pd.DataFrame(titulosdeprueba, columns = [\"Review\",\"Puntaje predicho\"])\n",
    "\n",
    "tabla_titulosdeprueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarizacion del problema y reducción de error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa buscaremos reducir el error y mejorar el score del modelo del anterior proyecto binarizando el problema. Para esto, en lugar de predecir un puntaje, predecirá si la review recomienda el producto o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos el dataset en 2\n",
    "dataset.loc[dataset[dataset['stars']< 3].index,'recomendable']=0\n",
    "dataset.loc[dataset[dataset['stars']>= 3].index,'recomendable']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomaremos que una review recomienda un producto cuando supuntaje sea mayor o igual a 3 estrellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso valores a bool\n",
    "dataset = dataset.astype({\"recomendable\": bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con los nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['review_title']\n",
    "y = dataset['recomendable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bin = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf_bin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya viendo la matriz de confusión ya se ve una mejora pero veamos que dice el score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bin.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset[dataset['stars']< 3].index,'recomendable'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bin = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bin['recom_pred'] = clf_bin.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos como quedo la predicción de si las review recomiendan el producto o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(dataset_bin['recom_pred'].value_counts().index,dataset_bin['recom_pred'].value_counts().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews negativas relacionadas con envio\n",
    "<br>\n",
    "Tambien nos preguntamos ¿Cuantas reviews de productos tienen un puntaje negativo debido a demoras o problemas ne la entrega del producto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos algoritmos <code>Word2Vec</code> y <code>FastText</code> para medir Word Mover’s Distance o <code>WMD</code>. Esto nos va a servir para obtener la distancia entre palabras u oraciones y distinguir cuales se refieren a entregas rapidas y lentas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo FastText con sbwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wordvectors_file_vec = 'fasttext-sbwc.3.6.e20.vec'\n",
    "cantidad = 100000\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['demora'])#probamos palabras cercanas a demora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frases distintas\n",
    "frase1=\"el paquete llego rapido\"\n",
    "frase2=\"hubo demora en el envio\"\n",
    "\n",
    "distancia = wordvectors.wmdistance(frase1, frase2)\n",
    "print('Distancia = %.4f' % distancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frases parecidas\n",
    "frase1=\"el paquete llego con demora\"\n",
    "frase2=\"hubo demora en el envio\"\n",
    "\n",
    "distancia = wordvectors.wmdistance(frase1, frase2)\n",
    "print('Distancia = %.4f' % distancia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que el ultimo par de oraciones tiene una similitud el algoritmo lo marca como lejano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo w2v con sbwc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos como se desempeña con W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBW_vectors_file = 'SBW-vectors-300-min5.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelow2v = KeyedVectors.load_word2vec_format(SBW_vectors_file, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frases distintas\n",
    "frase1= \"No han entregado el producto\"\n",
    "frase2= \"el envio llego rapido\"\n",
    "\n",
    "distance = modelow2v.wmdistance(frase1, frase2)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frases parecidas\n",
    "frase1= \"Plazo de envio no cumplido\"\n",
    "frase2= \"hubo demora en el envio\"\n",
    "\n",
    "distance = modelow2v.wmdistance(frase1, frase2)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pueden ver, a pesar de que el corpus con el que se entrenó sea en español y el modelo está pre entrenado para funcionar en español los resultados distan de ser buenos para empezar a clasificar una review está relacionada con una entrega.\n",
    "Sería interesante probar los resultados con un dataset en ingles, donde probablemente sea mejor el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por eso mismo probamos lemmatizando y haciendo busquedas tradicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar reviews negativas relacionadas con envio usando lemmatización\n",
    "<br>\n",
    "El objetivo sigue siendo encontrar las reviews que tienen mala puntuación y esté relacionada a la entrega del producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traemos las reviews que contengan la palabra <code>\"entrega\"</code> o <code>\"envio\"</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetprueba = dataset_bin[dataset_bin['review_title'].str.contains(\"entrega\") | dataset_bin['review_title'].str.contains(\"envio\")]\n",
    "datasetprueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el resultado es bastante bajo. ¿Que sucede si normalizamos y lemmatizamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_review_lemma = [] #lista donde se van a guardar las reviews lemmatizadas\n",
    "for titulo in dataset_bin.review_body:\n",
    "    dataset_review_lemma.append(\" \".join(token_cleaner(titulo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bin.insert(4,\"review_title_lemma\", dataset_review_lemma) #agregamos las reviews lemmatizadas al dataframe\n",
    "dataset_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las busqueda en base a palabras que estén relacionadas con la entrega y la demor del producto luego de lematizar.\n",
    "<br>\n",
    "Hay palabras que están incompletas para tambien se cuenten las conjugación de estas mismas que la lemmatización no pudo reducir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "palabras_clave = [\"entreg\",\"envi\",\"lento\",\"demora\",\"tardar\"]#lista de palabras \n",
    "dataset_envios_tarde = dataset_bin[(dataset_bin['review_title_lemma'].str.contains('|'.join(palabras_clave))) & (dataset_bin.recomendable==False)]\n",
    "dataset_envios_tarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado encontrado fue casi 7 mayor una vez lematizados los <code>review_body</code> de cada review.\n",
    "<br>\n",
    "Lematizamos <code>review_body</code> ya que luego de pruebas daba mejores resultados que <code>review_titley</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder visualizar un poco mejor vamos a graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores=[dataset_envios_tarde.shape[0],dataset_bin[dataset_bin[\"recomendable\"]==0].shape[0]-dataset_envios_tarde.shape[0]\n",
    "-dataset_envios_tarde.shape[0]]\n",
    "\n",
    "colores = ['#DD7596', '#8EB897']# Colores para el grafico\n",
    "\n",
    "ax = plt.subplots(figsize=[10,6])\n",
    "plt.pie(valores,labels=[\"Reviews negativas relacionadas con envio\",\"Resto de reviews negativas\"],\n",
    "        labeldistance=1.15, wedgeprops = { 'linewidth' : 1, 'edgecolor' : 'black' }, colors=colores,\n",
    "        explode=(0.2, 0),autopct=\"%.1f%%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos aproximadamente 7119 (9% de las 200.000 reviews) reviews negativas que se relacionan con la entrega del producto.\n",
    "<br>\n",
    "Este dato puede servir para analizar y mejorar la distribución de los productos, reduciendo la cantidad de clientes insatisfechos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
